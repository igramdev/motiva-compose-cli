import { z } from 'zod';
import { OpenAIWrapper } from './openai.js';
import chalk from 'chalk';
import dotenv from 'dotenv';

// .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
dotenv.config();

export interface LLMRequest {
  model: string;
  systemPrompt: string;
  userInput: string;
  temperature?: number;
  maxTokens?: number;
  responseFormat?: 'json_object' | 'text';
}

export interface LLMResponse<T> {
  data: T;
  tokensUsed: number;
  costUSD: number;
  provider: string;
  model: string;
  duration: number;
}

export interface LLMProvider {
  name: string;
  generateJSON<T>(
    request: LLMRequest,
    schema: z.ZodSchema<T>
  ): Promise<LLMResponse<T>>;
  
  generateText(request: LLMRequest): Promise<LLMResponse<string>>;
  
  isAvailable(): boolean;
  
  getSupportedModels(): string[];
}

/**
 * OpenAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…
 */
export class OpenAIProvider implements LLMProvider {
  name = 'openai';
  private wrapper: OpenAIWrapper;

  constructor(apiKey?: string) {
    // .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°å¼•æ•°ã¾ãŸã¯ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨
    const envApiKey = process.env.OPENAI_API_KEY;
    const finalApiKey = apiKey || envApiKey || 'mock';
    this.wrapper = new OpenAIWrapper(finalApiKey);
  }

  async generateJSON<T>(
    request: LLMRequest,
    schema: z.ZodSchema<T>
  ): Promise<LLMResponse<T>> {
    const startTime = Date.now();
    const response = await this.wrapper.generateJSON(request, schema);
    const duration = Date.now() - startTime;

    return {
      ...response,
      provider: this.name,
      model: request.model,
      duration
    };
  }

  async generateText(request: LLMRequest): Promise<LLMResponse<string>> {
    const startTime = Date.now();
    // ä¸€æ™‚çš„ã«generateJSONã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    // TODO: OpenAIWrapperã«generateTextãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
    const response = await this.wrapper.generateJSON(request, z.string());
    const duration = Date.now() - startTime;

    return {
      data: response.data,
      tokensUsed: response.tokensUsed,
      costUSD: response.costUSD,
      provider: this.name,
      model: request.model,
      duration
    };
  }

  isAvailable(): boolean {
    return !!process.env.OPENAI_API_KEY || process.env.NODE_ENV === 'test' || !process.env.OPENAI_API_KEY;
  }

  getSupportedModels(): string[] {
    return [
      'gpt-4o',
      'gpt-4o-mini',
      'gpt-4-turbo',
      'gpt-3.5-turbo',
      'gpt-4',
      'gpt-3.5-turbo-16k'
    ];
  }
}

/**
 * Anthropicãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
 */
export class AnthropicProvider implements LLMProvider {
  name = 'anthropic';

  async generateJSON<T>(
    request: LLMRequest,
    schema: z.ZodSchema<T>
  ): Promise<LLMResponse<T>> {
    throw new Error('Anthropic provider not yet implemented');
  }

  async generateText(request: LLMRequest): Promise<LLMResponse<string>> {
    throw new Error('Anthropic provider not yet implemented');
  }

  isAvailable(): boolean {
    return !!process.env.ANTHROPIC_API_KEY;
  }

  getSupportedModels(): string[] {
    return ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku'];
  }
}

/**
 * Groqãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
 */
export class GroqProvider implements LLMProvider {
  name = 'groq';

  async generateJSON<T>(
    request: LLMRequest,
    schema: z.ZodSchema<T>
  ): Promise<LLMResponse<T>> {
    throw new Error('Groq provider not yet implemented');
  }

  async generateText(request: LLMRequest): Promise<LLMResponse<string>> {
    throw new Error('Groq provider not yet implemented');
  }

  isAvailable(): boolean {
    return !!process.env.GROQ_API_KEY;
  }

  getSupportedModels(): string[] {
    return ['llama3-8b-8192', 'llama3-70b-8192', 'mixtral-8x7b-32768'];
  }
}

/**
 * Self-hosted Mistralãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
 */
export class SelfHostedMistralProvider implements LLMProvider {
  name = 'self-hosted-mistral';
  private baseUrl: string;

  constructor(baseUrl: string) {
    this.baseUrl = baseUrl;
  }

  async generateJSON<T>(
    request: LLMRequest,
    schema: z.ZodSchema<T>
  ): Promise<LLMResponse<T>> {
    throw new Error('Self-hosted Mistral provider not yet implemented');
  }

  async generateText(request: LLMRequest): Promise<LLMResponse<string>> {
    throw new Error('Self-hosted Mistral provider not yet implemented');
  }

  isAvailable(): boolean {
    return true; // å¸¸ã«åˆ©ç”¨å¯èƒ½ï¼ˆè¨­å®šä¾å­˜ï¼‰
  }

  getSupportedModels(): string[] {
    return ['mistral-7b-instruct', 'mistral-large', 'mixtral-8x7b'];
  }
}

/**
 * LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†ã‚¯ãƒ©ã‚¹
 */
export class LLMProviderManager {
  private providers: Map<string, LLMProvider> = new Map();
  private defaultProvider: string = 'openai';

  constructor() {
    this.registerDefaultProviders().catch(error => {
      console.error(chalk.red('âŒ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç™»éŒ²ã‚¨ãƒ©ãƒ¼:'), error);
    });
  }

  private async registerDefaultProviders(): Promise<void> {
    // OpenAIï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    const openaiProvider = new OpenAIProvider();
    this.providers.set('openai', openaiProvider);

    // Anthropic
    const anthropicProvider = new AnthropicProvider();
    this.providers.set('anthropic', anthropicProvider);

    // Mock Providerï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    if (process.env.NODE_ENV === 'test' || !process.env.OPENAI_API_KEY) {
      try {
        const { MockLLMProvider } = await import('./mock-provider.js');
        const mockProvider = new MockLLMProvider();
        this.providers.set('mock', mockProvider);
        console.log(chalk.green('âœ… ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ã—ã¾ã—ãŸ'));
      } catch (error) {
        console.log(chalk.yellow('âš ï¸  ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—'));
      }
    }

    // Groq
    const groqProvider = new GroqProvider();
    this.providers.set('groq', groqProvider);

    // Self-hosted Mistralï¼ˆè¨­å®šã‹ã‚‰ï¼‰
    if (process.env.MISTRAL_BASE_URL) {
      const mistralProvider = new SelfHostedMistralProvider(process.env.MISTRAL_BASE_URL);
      this.providers.set('self-hosted-mistral', mistralProvider);
    }
  }

  /**
   * ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²
   */
  registerProvider(name: string, provider: LLMProvider): void {
    this.providers.set(name, provider);
    console.log(chalk.green(`âœ… LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç™»éŒ²: ${name}`));
  }

  /**
   * åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
   */
  getAvailableProviders(): LLMProvider[] {
    return Array.from(this.providers.values()).filter(p => p.isAvailable());
  }

  /**
   * ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
   */
  getProvider(name?: string): LLMProvider {
    const providerName = name || this.defaultProvider;
    const provider = this.providers.get(providerName);
    
    if (!provider) {
      throw new Error(`ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ${providerName}`);
    }

    if (!provider.isAvailable()) {
      throw new Error(`ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: ${providerName}`);
    }

    return provider;
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•é¸æŠ
   */
  getProviderForModel(model: string): LLMProvider {
    for (const provider of this.providers.values()) {
      if (provider.isAvailable() && provider.getSupportedModels().includes(model)) {
        return provider;
      }
    }

    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿”ã™
    return this.getProvider();
  }

  /**
   * åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—
   */
  getAvailableModels(): Array<{ provider: string; model: string }> {
    const models: Array<{ provider: string; model: string }> = [];
    
    for (const provider of this.providers.values()) {
      if (provider.isAvailable()) {
        provider.getSupportedModels().forEach(model => {
          models.push({ provider: provider.name, model });
        });
      }
    }

    return models;
  }

  /**
   * ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
   */
  showProviderInfo(): void {
    console.log(chalk.blue('ğŸ¤– åˆ©ç”¨å¯èƒ½ãªLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:'));
    
    for (const provider of this.providers.values()) {
      const status = provider.isAvailable() ? chalk.green('âœ…') : chalk.red('âŒ');
      console.log(`  ${status} ${provider.name}`);
      
      if (provider.isAvailable()) {
        const models = provider.getSupportedModels().slice(0, 3).join(', ');
        console.log(`    ğŸ“‹ ãƒ¢ãƒ‡ãƒ«: ${models}${provider.getSupportedModels().length > 3 ? '...' : ''}`);
      }
    }
  }
}

// ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
export const llmProviderManager = new LLMProviderManager(); 